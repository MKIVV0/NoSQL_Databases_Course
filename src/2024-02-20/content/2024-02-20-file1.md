# Addressing the need for NoSQL Databases
## Why opt for Non-Relational Databases?
In an era increasingly driven by technology, the sheer<sub><a id="footnote-1-ref" href=#footnote-1>[1]</a></sub> **volume of data** processed by devices, such as computers, is experiencing **exponential growth**. This prompts a critical question: <strong>why do we need a class of databases that diverges from traditional relational models?</strong>
<br><br>
Given the vast volume of information processed and exchanged by computers in today's world, originating from **diverse sources** and exhibiting **various structures**, there's a compelling need for systems that are fast, reliable, and efficient in handling such diverse datasets.
<br><br>
In such a case, we can use the term <strong>Big Data</strong> to refer to information that contains greater variety arriving in increasing volumes and with ever-higher velocity. In particular, this term refers to datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze. We have, therefore, three dimensions that characterize this class of data:
<ul>
    <li><strong>Variety</strong>, which refers to the diverse types of data that can be encountered. This includes structured data (e.g., databases), unstructured data (e.g., text, images, videos), and semi-structured data (e.g., XML and JSON files).</li>
    <li><strong>Volume</strong>, which represent the sheer size or scale of the data being generated or collected. It refers to the vast amounts of data that exced the capacity of traditional database systems.</li>
    <li><strong>Velocity</strong>, the speed at which data is generated, processed, and made available for analysis. It emphasizes the real-time or near-real-time nature of data streams.</li>
</ul>
The following image shows a diagram comparing **big data** and **normal processing capabilities**. It depicts the vast difference in the amount of data that big data can handle compared to normal processing capacity:
<br><br>
<div style="text-align:center">
  <img src="../img/Big%20Data%20vs%20Normal%20Data.png" alt="Big Data vs Normal Data" style="width:80%; max-width:600px;">
</div>

## How can we quantify big data?
The **overwhelming volume** of data is a key aspect in big data. Traditionally, datasets on the order of **terabytes (TB)**, **petabytes (PB)**, or even **zettabytes (ZB)**, are considered as falling into the realm of big data.
For comparison purposes, here is the **byte scale**:
- \\(1\ Zettabyte = 2^{70}\ bytes\\)
- \\(1\ Exabyte = 2^{60}\ bytes\\)
- \\(1\ Petabyte = 2^{50}\ bytes\\)
- \\(1\ Terabyte = 2^{40}\ bytes\\)
- \\(1\ Gigabyte = 2^{30}\ bytes\\) 
- \\(1\ Megabyte = 2^{20}\ bytes\\) 
- \\(1\ Kilobyte = 2^{10}\ bytes\\) 
- \\(1\ Byte = 2^{3}\ bits\\) 

These values illustrate the increasing magnitudes of data storage.
The following image shows how the global generation of data increases annually:
<div style="text-align:center">
    <img src="../img/Global Data Generated Annually.png" alt="Global Data Generated Annually" style="width:80%; max-width:600px">
    <p>Reference: <a href="https://explodingtopics.com/blog/data-generated-per-day"> explodingtopics.com (2024)</a></p>
</div>

## What kind of data are we considering?
As mentioned before, data can be classified into three categories:
- **Structured Data** refers to well-organized, tabular data with a clear schema. This includes data stored in relational databases and spreadsheets. Big data applications may involve large volumes of structured data, especially when dealing with historical records, transactional data, or standardized information. <br>
- **Semi-structured Data** is a class of data that has some level of structure, but does not conform to the rigid structure of traditional relational databases. It may have tags, hierarchies, or key-value pairs. JSON and XML are included in this class. Big data applications may leverage semi-structured data formats for flexibility and scalability, such as handling data with varying attributes or evolving schemas. <br>
- **Unstructured Data** is a category of data that lacks a predefined data model or is not organized in a tabular structure. It includes text, images, audio, and video. Big data applications often deal with massive amounts of unstructured data, such as sentiment analysis on social media, image recognition, or natural language processing.

In fact, the following image shows the increasing treatment of unstructured data over structured data, as time goes on:
<div style="text-align:center">
    <img src="../img/Unstructured Data vs Structured Data.jpg" alt="Unstructured Data vs Structured Data" style="width:80%; max-width:600px">
    <p>Reference: <a href="https://e.huawei.com/en/articles/storage/2021/all-flash-data-center-green-energy">Huawei GIV (2021)</a></p>
</div>

As for Big Data sources, they comprise **social networks**, **logs of various web/email servers or routers**, **sensor networks**, **IoT devices**.

## Data Processing
In the expansive landscape of big data, efficient data processing is achieved through the orchestration of three key paradigms:
- **OLTP (Online Transaction Processing)**: it's a type of data processing that consists of executing a number of **transactions** occuring **concurrently**, such as **online banking**, **shopping**, **order entry**, or sending **text messages**. These transactions traditionally are referred to as **economic or financial** transactions, recorded and secured so that an enterprise can access the information anytime for **accounting or reporting** issues. In short, it enables the **real-time execution** of large numbers of transactions by large numbers of people;

- **OLAP (Online Analytical Processing)**: it focuses on **complex queries** and **data analysis**. OLAP systems are optimized for **analytical and ad-hoc queries**, involving aggregations, grouping, and multidimensional analysis. They are **read-heavy**, designed for **data warehousing** and **business intelligence** applications, typically using a **denormalized**<sub><a id="footnote-2-ref" href=#footnote-2></a></sub> data model to improve query performance.

- **RTAP (Real-Time Analytics Processing)**: aiming to provide **real-time analytical** capabilities on transactional data, RTAP systems offer **near-real-time analysis** on **streaming or transactional** data. These systems often have a **hybrid architecture** combining elements of OLTP and OLAP, enabling fast and analytical processing on **live data**. RTAP systems are designed to handle **high-velocity data streams** and **scale horizontally**<sub><a id="horizontal-scaling-ref" href=#horizontal-scaling>[3]</a></sub> to accomodate growing data volumes.

## Technologies for Big Data
As of today, we have different means to handle and manage **big data**:
- **Data Management Systems**, i.e. NoSQL databases;
- **Models for distributed programming**, i.e. MapReduce and others;
- **Grid and cloud computing**, e.g AWS;
- **Large-Scale Machine Learning**, e.g. Hadoop, Tensorflow and Pytorch.

This course focuses, as mentioned before and as the book title states, on NoSQL databases. The other cited elements will be treated in the <a href="https://uninsubria.coursecatalogue.cineca.it/insegnamenti/2024/25783/2018/9999/10088?coorte=2023&schemaid=5367">Cloud Computing course</a>, which will be conducted next year by the same lecturer of this course.

## Notes
<p id="footnote-1"><a href=#footnote-1-ref>[1]</a> With the term <b>sheer</b> we're emphasizing the <b>magnitude</b>, the <b>size of the data</b>, without introducing qualifiers or restrictions.

<p id="footnote-2"><a href=#footnote-2-ref>[2]</a> In a <b>normalized</b> data model, data is organized to <b>reduce redundancy</b> and <b>improve data integrity</b>. This is achieved by breaking down data into tables and establishing relationships between the two, which can involve splitting the data into separate tables and linking them with foreign keys. On the other hand, a <b>denormalized</b> data model involves combining tables and information to <b>simplify queries</b> and <b>increase read performance</b>. This means that <b>redundant data</b> may be stored in multiple places to <b>avoid joins and increase query speed</b>. While this can improve query performance, it may <b>sacrifice</b> some aspects of data integrity and can lead to data redundancy.</p>

<p id="horizontal-scaling"><a href="#horizontal-scaling-ref">[3]</a> <b>Horizontal scaling</b>, <b>or scale-out</b>, is a method of <b>increasing system capacity</b> by <b>adding more independent machines or nodes</b> to <b>distribute and handle workloads</b>. This approach <b>enhances redundancy</b>, offers <b>linear scalability</b>, and is <b>cost-effective</b>, commonly facilitated by cloud computing and containerization technologies. <b>Challenges</b> include ensuring <b>data consistency</b> and <b>effective workload distribution</b>. Notable examples include adding machines to a network or using container orchestration tools like Kubernetes.</p>